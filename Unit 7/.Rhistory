MeanAccIris = colMeans(masterAccIris)
print(MeanAccIris)
set.seed(2)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM =confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
CM$byClass['Sensitivity']
iterations = 100
masterAccIris = matrix(nrow = iterations, ncol = 3)
for(j in 1:iterations){
set.seed(j)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM = confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
masterAccIris[j,1] = CM$overall[['Accuracy']]
masterAccIris[j,2] = CM$byClass['Sensitivity']
masterAccIris[j,3] = CM$byClass['Specificity']
}
MeanAccIris = colMeans(masterAccIris)
print(MeanAccIris)
set.seed(2)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM =confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
CM$byClass
iterations = 100
masterAccIris = matrix(nrow = iterations, ncol = 3)
for(j in 1:iterations){
set.seed(j)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM = confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
masterAccIris[j,1] = CM$overall[['Accuracy']]
masterAccIris[j,2] = CM$byClass['Sensitivity']
masterAccIris[j,3] = CM$byClass['Specificity']
}
MeanAccIris = colMeans(masterAccIris)
print(MeanAccIris)
set.seed(2)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM =confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
CM$byClass['Sensitivity']
iterations = 100
masterAccIris = matrix(nrow = iterations, ncol = 3)
for(j in 1:iterations){
set.seed(j)
trainIndices = sample(seq(1:length(iris$Sepal.Length)),round(.7*length(iris$Sepal.Length)))
trainIris = iris[trainIndices,]
testIris = iris[-trainIndices,]
model = naiveBayes(trainIris[,c(1,2)],trainIris$Species)
CM = confusionMatrix(table(predict(model,testIris[,c(1,2)]),testIris$Species))
masterAccIris[j,1] = CM$overall[['Accuracy']]
masterAccIris[j,2] = CM$byClass['Sensitivity']
masterAccIris[j,3] = CM$byClass['Specificity']
}
MeanAccIris = colMeans(masterAccIris)
print(MeanAccIris)
library(tm) #text mining library provides the stopwords() function
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(class)
library(caret)
library(e1071)
library(ggplot2)
library(plotly)
NYTIMES_KEY = "FLATz6I7PRFS23jHX88567oG0Al46BW8" #Your Key Here … get from NTY API website
# Let's set some parameters
term <- "Central+Park+Jogger" # Need to use + to string together separate words
begin_date <- "19900419"
end_date <- "19900619"
#Alternate search terms
term <- "Trump"
begin_date <- "20200101"
end_date <- "20200131"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl
initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages
pages <- list()
for(i in 0:maxPages){
nytSearch <- jsonlite::fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(7)
}
allNYTSearch <- rbind_pages(pages)
allNYTSearch$response.docs.type_of_material
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
#Make another column of News versus Other ... The labels
allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
#There is an NA in NewsOrOther
# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>%
group_by(NewsOrOther) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()
allNYTSearch$response.docs.type_of_material
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
#Make another column of News versus Other ... The labels
allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
#There is an NA in NewsOrOther
# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>%
group_by(NewsOrOther) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()
dim(allNYTSearch)[1]
allNYTSearch
allNYTSearch
allNYTSearch$response.docs.headline.main
allNYTSearch$response.docs.snippet
set.seed(2)
trainInd = sample(seq(1,dim(allNYTSearch)[1],1),round(.7*dim(allNYTSearch)[1]))
allNYTSearchTrain = allNYTSearch[trainInd,]
allNYTSearchTest = allNYTSearch[-trainInd,]
#This function returns P(News | Keyword)
#P(News|KW) = P(KW|News)* P(News) / P(KW)
Pnews_word_headline = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
trainingSet$response.docs.headline.main = unlist(str_replace_all(trainingSet$response.docs.headline.main,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
#print(key_word)
NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
pOther = 1 - pNews
pKWGivenNews = (length(str_which(NewsGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
pKWGivenOther = (length(str_which(OtherGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
pKW = length(str_which(trainingSet$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
pNewsGivenKW = pKWGivenNews*pNews/pKW
pOtherGivenKW = pKWGivenOther*pOther/pKW
return(pNewsGivenKW)
}
Pnews_word_snippet = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
trainingSet$response.docs.snippet = unlist(str_replace_all(trainingSet$response.docs.snippet,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
#print(key_word)
NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
pOther = 1 - pNews
pKWGivenNews = (length(str_which(NewsGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
pKWGivenOther = (length(str_which(OtherGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
pKW = length(str_which(trainingSet$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
pNewsGivenKW = pKWGivenNews*pNews/pKW
pOtherGivenKW = pKWGivenOther*pOther/pKW
return(pNewsGivenKW)
}
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
}
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
# Classify the aricle as News or Other based on a given piece of information from the article.
theScoreHolderNews
theScoreHolderOther
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")
head(allNYTSearchTest)
#Confusion Matrix
tableNYT <- table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther)
#confusionMatrix(tableNYT)
#Actual in Columns
confusionMatrix(factor(allNYTSearchTest$Classified),factor(allNYTSearchTest$NewsOrOther))
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
}
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
}
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
}
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word_headline(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word_headline(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
# Classify the aricle as News or Other based on a given piece of information from the article.
theScoreHolderNews
theScoreHolderOther
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")
head(allNYTSearchTest)
#Confusion Matrix
tableNYT <- table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther)
#confusionMatrix(tableNYT)
#Actual in Columns
confusionMatrix(factor(allNYTSearchTest$Classified),factor(allNYTSearchTest$NewsOrOther))
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
}
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word_headline(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word_headline(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
install.packages("pairwiseCI")
library('pairwiseCI')
install.packages("pairwiseCI")
install.packages("multcomp")
library(pairwiseCI)
library(multcomp)
install.packages("pairwiseCI")
#install.packages("pairwiseCI")
install.packages("multcomp")
library(pairwiseCI)
library(multcomp)
Handicap <- read.csv(file.choose(), header = TRUE)
pairwiseCI(Score~Handicap, data = Handicap)
pairwiseCI(Score~Handicap, data = Handicap, title='Differences of Means')
pairwiseCI(Score~Handicap, data = Handicap, main='Differences of Means')
pairwiseCI(Score~Handicap, data = Handicap, labels('Diff'))
pairwiseCI(Score~Handicap, data = Handicap)
glht(fit, linfct = mcp(Handicap = "Tukey"))
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
pairwiseCI(Score~Handicap, data = Handicap)
#Setup of Fit
fit=aov(Score~Handicap, data = Handicap)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
summary(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "LSD"))
gfit = glht(fit, linfct = mcp(Handicap = "lsd"))
gfit = glht(fit, linfct = mcp(Handicap = "dunnet"))
gfit = glht(fit, linfct = mcp(Handicap = "Dunnet"))
confint(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Lsd"))
LSD.test(fit,"b", group=TRUE)
#install.packages("pairwiseCI")
#install.packages("multcomp")
install.packages("agricolae")
library(pairwiseCI)
library(multcomp)
library(agricolae)
LSD.test(fit,"b", group=TRUE)
gfit = glht(fit, linfct = mcp(Handicap = "Lsd"))
LSD.test(fit,"b", group=TRUE)
library(agricolae)
comp1 <- LSD.test(fit,"a", group=FALSE)
comp2 <- LSD.test(fit,"b", group=TRUE)
# interaction ab
# Tukey's test
comp3 <- HSD.test(fit,"ab")
# graphics
par(mfrow=c(2,2))
bar.err(comp1,ylim=c(0,100), col="yellow")
bar.group(comp2,ylim=c(0,100),density=4,col="blue")
LSD.test(fit,"b", group=TRUE)
library(agricolae)
comp1 <- LSD.test(fit,"a", group=FALSE)
comp2 <- LSD.test(fit,"Handicap", group=TRUE)
# interaction ab
# Tukey's test
comp3 <- HSD.test(fit,"Handicap")
# graphics
par(mfrow=c(2,2))
bar.err(comp1,ylim=c(0,100), col="yellow")
bar.group(comp2,ylim=c(0,100),density=4,col="blue")
LSD.test(fit,"b", group=TRUE)
library(agricolae)
comp1 <- LSD.test(fit,"Handicap", group=FALSE)
comp2 <- LSD.test(fit,"Handicap", group=TRUE)
# interaction ab
# Tukey's test
comp3 <- HSD.test(fit,"Handicap")
# graphics
par(mfrow=c(2,2))
bar.err(comp1,ylim=c(0,100), col="yellow")
LSD.test(fit,"b", group=TRUE)
library(agricolae)
comp1 <- LSD.test(fit,"Handicap", group=FALSE)
comp2 <- LSD.test(fit,"Handicap", group=TRUE)
# interaction ab
# Tukey's test
comp3 <- HSD.test(fit,"Handicap")
# graphics
par(mfrow=c(2,2))
comp1
comp2
comp3
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
# Tukey's test
tukey <- LSD.test(fit,"Handicap")
tukey
LSD.test(fit,"b", group=TRUE)
library(agricolae)
LSD <- LSD.test(fit,"Handicap", group=FALSE)
LSD
library(agricolae)
LSD <- LSD.test(fit,"Handicap", group=FALSE)
LSD
gfit = glht(fit, linfct = mcp(Handicap = "Dunnet"))
confint(gfit)
summary.lm(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Dunnet"))
confint(gfit)
summary(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Dunnet"))
confint(gfit)
summary.table(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
TukeyHSD
# Tukey's test
tukey <- TukeyHSD(fit,"Handicap")
tukey
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
TukeyHSD
# Tukey's test
tukey <- TukeyHSD(fit,"Handicap")
confint(tukey)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
TukeyHSD
# Tukey's test
tukey <- TukeyHSD(fit,"Handicap")
summary(tukey)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
TukeyHSD
# Tukey's test
TukeyHSD(fit,"Handicap")
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
#confint(gfit)
# Tukey's test
TukeyHSD(fit,"Handicap")
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
# Tukey's test
TukeyHSD(fit,"Handicap")
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
# Tukey's test
TukeyHSD(fit,"Handicap", ordered = TRUE)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
# Tukey's test
TukeyHSD(fit,"Handicap", ordered = TRUE)
Handicap$Handicap = relevel(Handicap$Handicap, ref = "None")
fit=aov(Score~Handicap, data = Handicap)
gfit = glht(fit, linfct = mcp(Handicap = "Dunnet"))
confint(gfit)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
confint(gfit)
# Tukey's test
TukeyHSD(fit,"Handicap", ordered = TRUE)
gfit = glht(fit, linfct = mcp(Handicap = "Bon"))
gfit = glht(fit, linfct = mcp(Handicap = "Bonf"))
gfit = glht(fit, linfct = mcp(Handicap = "Bf"))
gfit = glht(fit, linfct = mcp(Handicap = "Bonferonni"))
gfit = glht(fit, linfct = mcp(Handicap = "Bonferonni"))
fit.gh <- glht(fit, linfct = mcp(group = K))
fit.gh <- glht(fit, linfct = mcp(Handicap = K))
fit.gh <- glht(fit, linfct = mcp(Handicap = "K"))
fit.gh <- glht(fit, linfct = mcp(group = Handicap))
fit.gh <- glht(fit, linfct = mcp(Handicap))
fit.gh <- glht(fit, linfct = mcp(names = Handicap))
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
summary(gfit, test = adjusted("bonferroni"))
## Individual p-values
summary(fit.gh, test = adjusted("None"))
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
summary(gfit, test = adjusted("bonferroni"))
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
summary(gfit, test = adjusted("bonferroni"))
pairwise.t.test(Handicap$Score, Handicap$Handicap, p.adj = “bonferroni”)
gfit = glht(fit, linfct = mcp(Handicap = "Tukey"))
summary(gfit, test = adjusted("bonferroni"))
pairwise.t.test(Handicap$Score, Handicap$Handicap, p.adj = 'bonferroni')
PostHocTest(fit, method = "scheffe")
install.packages('DescTools')
library(DescTools)
PostHocTest(fit, method = "scheffe")
